{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (MBTI) Myers-Briggs Personality Type Prediction\n",
    "\n",
    "* Extroversion vs. Introversion\n",
    "    * I - 0\n",
    "    * E - 1 \n",
    "    \n",
    "* Sensing vs. Intuition \n",
    "    * N - 0 \n",
    "    * S - 1\n",
    "    \n",
    "* Thinking vs. Feeling\n",
    "    * F - 0\n",
    "    * T - 1\n",
    "    \n",
    "* Judging vs. Perceiving\n",
    "    * P - 0\n",
    "    * J - 1 \n",
    "    \n",
    "\n",
    "## SENTIMENT ANALYSIS & PART OF SPEECH TAGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing dependencies here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# feature engineering\n",
    "import re\n",
    "\n",
    "# pos tagging\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# sentiment scoring\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# scaling to handle negative values in sentiment scores (for Naive Bayes)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# performance check\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering - II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the clean_dataset_1\n",
    "personality_data = pd.read_csv(os.path.join(\"..\", \"data\", \"clean_data_1.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>is_Extrovert</th>\n",
       "      <th>is_Sensing</th>\n",
       "      <th>is_Thinking</th>\n",
       "      <th>is_Judging</th>\n",
       "      <th>posts</th>\n",
       "      <th>clean_posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>'When asked of the things you wish you did ear...</td>\n",
       "      <td>asked thing wish earlier       find answering...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>'I love both and they are equally important to...</td>\n",
       "      <td>love equally important  music window soul  in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Really? You think implying that everyone who i...</td>\n",
       "      <td>really  think implying everyone entrepreneur s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENFJ</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>'Love is a crazy thing.   Se is our best form ...</td>\n",
       "      <td>love crazy thing     best form communication ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INTP</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>'I am a physics undergrad with a computation e...</td>\n",
       "      <td>physic undergrad computation emphasis  learni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type  is_Extrovert  is_Sensing  is_Thinking  is_Judging  \\\n",
       "0  INFJ             0           0            0           1   \n",
       "1  INFJ             0           0            0           1   \n",
       "2  INFJ             0           0            0           1   \n",
       "3  ENFJ             1           0            0           1   \n",
       "4  INTP             0           0            1           0   \n",
       "\n",
       "                                               posts  \\\n",
       "0  'When asked of the things you wish you did ear...   \n",
       "1  'I love both and they are equally important to...   \n",
       "2  Really? You think implying that everyone who i...   \n",
       "3  'Love is a crazy thing.   Se is our best form ...   \n",
       "4  'I am a physics undergrad with a computation e...   \n",
       "\n",
       "                                         clean_posts  \n",
       "0   asked thing wish earlier       find answering...  \n",
       "1   love equally important  music window soul  in...  \n",
       "2  really  think implying everyone entrepreneur s...  \n",
       "3   love crazy thing     best form communication ...  \n",
       "4   physic undergrad computation emphasis  learni...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at the top 5 rows of the dataset\n",
    "personality_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8588, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the number of rows and columns\n",
    "personality_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type            0\n",
       "is_Extrovert    0\n",
       "is_Sensing      0\n",
       "is_Thinking     0\n",
       "is_Judging      0\n",
       "posts           0\n",
       "clean_posts     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for missing values\n",
    "personality_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no missing values present in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiments Analysis Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CAUTION - Sentiment scoring will take LONG !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Scoring Time: 156.60 seconds\n"
     ]
    }
   ],
   "source": [
    "# sentiment scoring for each user\n",
    "t = time.time()\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "nlp_sentiment_score = []\n",
    "\n",
    "for post in personality_data[\"clean_posts\"]:\n",
    "    score = analyzer.polarity_scores(post)\n",
    "    nlp_sentiment_score.append(score)\n",
    "\n",
    "print(f\"Sentiment Scoring Time: {time.time() - t:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segregating the indiviual sentiment scores - compound, positive, negative and neutral\n",
    "personality_data[\"compound_sentiment\"] = [\n",
    "    score[\"compound\"] for score in nlp_sentiment_score\n",
    "]\n",
    "personality_data[\"pos_sentiment\"] = [score[\"pos\"] for score in nlp_sentiment_score]\n",
    "personality_data[\"neg_sentiment\"] = [score[\"neg\"] for score in nlp_sentiment_score]\n",
    "personality_data[\"neu_sentiment\"] = [score[\"neu\"] for score in nlp_sentiment_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment scores have negative values that Naive Bayes can't handle. So scaling it.\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "personality_data[\"compound_sentiment\"] = min_max_scaler.fit_transform(\n",
    "    np.array(personality_data[\"compound_sentiment\"]).reshape(-1, 1)\n",
    ")\n",
    "personality_data[\"pos_sentiment\"] = min_max_scaler.fit_transform(\n",
    "    np.array(personality_data[\"pos_sentiment\"]).reshape(-1, 1)\n",
    ")\n",
    "personality_data[\"neg_sentiment\"] = min_max_scaler.fit_transform(\n",
    "    np.array(personality_data[\"neg_sentiment\"]).reshape(-1, 1)\n",
    ")\n",
    "personality_data[\"neu_sentiment\"] = min_max_scaler.fit_transform(\n",
    "    np.array(personality_data[\"neu_sentiment\"]).reshape(-1, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type                  0\n",
       "is_Extrovert          0\n",
       "is_Sensing            0\n",
       "is_Thinking           0\n",
       "is_Judging            0\n",
       "posts                 0\n",
       "clean_posts           0\n",
       "compound_sentiment    0\n",
       "pos_sentiment         0\n",
       "neg_sentiment         0\n",
       "neu_sentiment         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking to see if sentiment scores introduced any null value\n",
    "personality_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating tag_posts column that will have each post as a separate list in a row. tag_posts will be a list of 50 lists.\n",
    "\n",
    "# replacing urls with domain name\n",
    "personality_data[\"tag_posts\"] = personality_data[\"posts\"].str.replace(\n",
    "    re.compile(r\"https?:\\/\\/(www)?.?([A-Za-z_0-9-]+)([\\S])*\"),\n",
    "    lambda match: match.group(2),\n",
    ")\n",
    "\n",
    "# replacing ||| with space\n",
    "personality_data[\"tag_posts\"] = [\n",
    "    post for post in personality_data[\"tag_posts\"].str.split(\"\\|\\|\\|\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CAUTION - The next step i.e. Parts of speech tagging for each word will take SUPER LONG !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tagging Time: 328.23310899734497 seconds\n"
     ]
    }
   ],
   "source": [
    "# parts of speech tagging for each word\n",
    "t = time.time()\n",
    "\n",
    "personality_data[\"tagged_words\"] = personality_data[\"tag_posts\"].apply(\n",
    "    lambda x: [nltk.pos_tag(word_tokenize(line)) for line in x]\n",
    ")\n",
    "\n",
    "print(f\"POS Tagging Time: {time.time() - t} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating list of unique POS tags\n",
    "tag_set = set()\n",
    "\n",
    "for i, data in personality_data[\"tagged_words\"].iteritems():\n",
    "    for tup in data[0]:\n",
    "        tag_set.add(tup[1])\n",
    "\n",
    "tag_list = list(tag_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Stats Time: 43.5128231048584 seconds\n"
     ]
    }
   ],
   "source": [
    "# calculating mean and standard deviation of pos tags for each user\n",
    "t = time.time()\n",
    "\n",
    "\n",
    "def pos_cat(x, tag):\n",
    "    return [len([y for y in line if y[1] == tag]) for line in x]\n",
    "\n",
    "\n",
    "for col in tag_list:\n",
    "    personality_data[\"POS_\" + col + \"_mean\"] = personality_data[\"tagged_words\"].apply(\n",
    "        lambda x: np.mean(pos_cat(x, col))\n",
    "    )\n",
    "    personality_data[\"POS_\" + col + \"_std\"] = personality_data[\"tagged_words\"].apply(\n",
    "        lambda x: np.std(pos_cat(x, col))\n",
    "    )\n",
    "\n",
    "print(f\"POS Stats Time: {time.time() - t} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouping pos tags based on stanford list\n",
    "tags_dict = {\n",
    "    \"ADJ\": [\"JJ\", \"JJR\", \"JJS\"],\n",
    "    \"ADP\": [\"EX\", \"TO\"],\n",
    "    \"ADV\": [\"RB\", \"RBR\", \"RBS\", \"WRB\"],\n",
    "    \"CONJ\": [\"CC\", \"IN\"],\n",
    "    \"DET\": [\"DT\", \"PDT\", \"WDT\"],\n",
    "    \"NOUN\": [\"NN\", \"NNS\", \"NNP\", \"NNPS\"],\n",
    "    \"NUM\": [\"CD\"],\n",
    "    \"PRT\": [\"RP\"],\n",
    "    \"PRON\": [\"PRP\", \"PRP$\", \"WP\", \"WP$\"],\n",
    "    \"VERB\": [\"MD\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"],\n",
    "    \".\": [\"#\", \"$\", \"''\", \"(\", \")\", \",\", \".\", \":\"],\n",
    "    \"X\": [\"FW\", \"LS\", \"UH\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_9/zt66nrqx6rg37b3mvyxflk740000gn/T/ipykernel_81714/2397153783.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  personality_data[col + \"_avg\"] = personality_data[\"tagged_words\"].apply(\n",
      "/var/folders/_9/zt66nrqx6rg37b3mvyxflk740000gn/T/ipykernel_81714/2397153783.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  personality_data[col + \"_avg\"] = personality_data[\"tagged_words\"].apply(\n",
      "/var/folders/_9/zt66nrqx6rg37b3mvyxflk740000gn/T/ipykernel_81714/2397153783.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  personality_data[col + \"_avg\"] = personality_data[\"tagged_words\"].apply(\n",
      "/var/folders/_9/zt66nrqx6rg37b3mvyxflk740000gn/T/ipykernel_81714/2397153783.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  personality_data[col + \"_avg\"] = personality_data[\"tagged_words\"].apply(\n",
      "/var/folders/_9/zt66nrqx6rg37b3mvyxflk740000gn/T/ipykernel_81714/2397153783.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  personality_data[col + \"_avg\"] = personality_data[\"tagged_words\"].apply(\n",
      "/var/folders/_9/zt66nrqx6rg37b3mvyxflk740000gn/T/ipykernel_81714/2397153783.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  personality_data[col + \"_avg\"] = personality_data[\"tagged_words\"].apply(\n",
      "/var/folders/_9/zt66nrqx6rg37b3mvyxflk740000gn/T/ipykernel_81714/2397153783.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  personality_data[col + \"_avg\"] = personality_data[\"tagged_words\"].apply(\n",
      "/var/folders/_9/zt66nrqx6rg37b3mvyxflk740000gn/T/ipykernel_81714/2397153783.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  personality_data[col + \"_avg\"] = personality_data[\"tagged_words\"].apply(\n",
      "/var/folders/_9/zt66nrqx6rg37b3mvyxflk740000gn/T/ipykernel_81714/2397153783.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  personality_data[col + \"_avg\"] = personality_data[\"tagged_words\"].apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stanford POS Stats Time: 12.406032085418701 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_9/zt66nrqx6rg37b3mvyxflk740000gn/T/ipykernel_81714/2397153783.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  personality_data[col + \"_avg\"] = personality_data[\"tagged_words\"].apply(\n"
     ]
    }
   ],
   "source": [
    "# Stanford POS tag stats\n",
    "t = time.time()\n",
    "\n",
    "\n",
    "def stanford_tag(x, tag):\n",
    "    tags_list = [len([y for y in line if y[1] in tags_dict[col]]) for line in x]\n",
    "    return tags_list\n",
    "\n",
    "\n",
    "for col in tags_dict.keys():\n",
    "    personality_data[col + \"_avg\"] = personality_data[\"tagged_words\"].apply(\n",
    "        lambda x: np.median(stanford_tag(x, col))\n",
    "    )\n",
    "\n",
    "print(f\"Stanford POS Stats Time: {time.time() - t} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>is_Extrovert</th>\n",
       "      <th>is_Sensing</th>\n",
       "      <th>is_Thinking</th>\n",
       "      <th>is_Judging</th>\n",
       "      <th>posts</th>\n",
       "      <th>clean_posts</th>\n",
       "      <th>compound_sentiment</th>\n",
       "      <th>pos_sentiment</th>\n",
       "      <th>neg_sentiment</th>\n",
       "      <th>...</th>\n",
       "      <th>ADV_avg</th>\n",
       "      <th>CONJ_avg</th>\n",
       "      <th>DET_avg</th>\n",
       "      <th>NOUN_avg</th>\n",
       "      <th>NUM_avg</th>\n",
       "      <th>PRT_avg</th>\n",
       "      <th>PRON_avg</th>\n",
       "      <th>VERB_avg</th>\n",
       "      <th>._avg</th>\n",
       "      <th>X_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>'When asked of the things you wish you did ear...</td>\n",
       "      <td>asked thing wish earlier       find answering...</td>\n",
       "      <td>0.99980</td>\n",
       "      <td>0.418667</td>\n",
       "      <td>0.136150</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>'I love both and they are equally important to...</td>\n",
       "      <td>love equally important  music window soul  in...</td>\n",
       "      <td>0.99995</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.134585</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 115 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   type  is_Extrovert  is_Sensing  is_Thinking  is_Judging  \\\n",
       "0  INFJ             0           0            0           1   \n",
       "1  INFJ             0           0            0           1   \n",
       "\n",
       "                                               posts  \\\n",
       "0  'When asked of the things you wish you did ear...   \n",
       "1  'I love both and they are equally important to...   \n",
       "\n",
       "                                         clean_posts  compound_sentiment  \\\n",
       "0   asked thing wish earlier       find answering...             0.99980   \n",
       "1   love equally important  music window soul  in...             0.99995   \n",
       "\n",
       "   pos_sentiment  neg_sentiment  ...  ADV_avg CONJ_avg DET_avg  NOUN_avg  \\\n",
       "0       0.418667       0.136150  ...      4.0      5.0     3.0       6.0   \n",
       "1       0.600000       0.134585  ...      3.0      5.0     2.0       5.0   \n",
       "\n",
       "   NUM_avg  PRT_avg  PRON_avg  VERB_avg  ._avg  X_avg  \n",
       "0      0.0      0.0       4.0       8.0    5.0    0.0  \n",
       "1      0.0      0.0       5.0       8.0    3.0    0.0  \n",
       "\n",
       "[2 rows x 115 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a quick look at the data\n",
    "personality_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment scoring & POS Tagging took long. So saving the scored & tagged file to save time in the next step.\n",
    "personality_data.to_csv(os.path.join(\"..\", \"data\", \"clean_data_2.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
